{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ohi/Documents/GitHub/ChataRewrite/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    # BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "import json\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import os\n",
    "\n",
    "@dataclass\n",
    "class DataClass:\n",
    "    MODEL_PATH = \"Qwen/Qwen2-0.5B-Instruct\"\n",
    "    MAX_LENGTH = 96\n",
    "    EPOCH = 3\n",
    "    LORA_RANK = 2\n",
    "    LORA_ALPHA = 4 * LORA_RANK\n",
    "    LORA_DROPOUT = 0.5\n",
    "    LORA_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "                    \"lm_head\"]\n",
    "    LR = 5e-5\n",
    "    ADAPTER_PATH = 'weights/LORA/checkpoint-1794'\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'mps'\n",
    "\n",
    "# Macbook MPS\n",
    "if DataClass.DEVICE == 'mps':\n",
    "    os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "    # Optional: Turnning off tokenizer parallelism to avoid stuck\n",
    "    # os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = AutoConfig.from_pretrained(\n",
    "    DataClass.MODEL_PATH,\n",
    "    trust_remote_code = True,\n",
    "    attn_implementation = 'eager', #'flash_attention_2'\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    DataClass.MODEL_PATH,\n",
    "    trust_remote_code = True\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# quant_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit = True,\n",
    "#     bnb_4bit_quant_type=\"n4f\",\n",
    "#     bnb4bit_compute_dtype=torch.bfloat16,\n",
    "#     bnb_4bit_use_double_quant=True\n",
    "# )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    DataClass.MODEL_PATH,\n",
    "    device_map=DataClass.DEVICE,\n",
    "    low_cpu_mem_usage=True,\n",
    "    # load_in_8bit=True,\n",
    "    # load_in_4bit=True,\n",
    "    attn_implementation='eager', #'flash_attention_2',\n",
    "    torch_dtype=torch.bfloat16, # NOTE: MPS does not support torch.bfloat16 finetuning\n",
    "    trust_remote_code=True,\n",
    "    # quantization_config=quant_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ohi/Documents/GitHub/ChataRewrite/.venv/lib/python3.12/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "Adapter loaded from: weights/LORA/checkpoint-1794\n"
     ]
    }
   ],
   "source": [
    "if DataClass.ADAPTER_PATH:\n",
    "    model.load_adapter(DataClass.ADAPTER_PATH)\n",
    "    print(\"Adapter loaded from:\", DataClass.ADAPTER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(input_text):\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    outputs = model.generate(\n",
    "        # **input_ids\n",
    "        max_new_tokens=100,\n",
    "        do_sample=False,\n",
    "        num_beams=1,\n",
    "        temperature=None,\n",
    "        top_k=None,\n",
    "        top_p=None,\n",
    "        input_ids=input_ids['input_ids'].to(DataClass.DEVICE),\n",
    "        attention_mask=input_ids['attention_mask'].to(DataClass.DEVICE)\n",
    "    )\n",
    "    # Only generate output\n",
    "    input_token_len = input_ids['input_ids'].shape[-1]\n",
    "    return tokenizer.decode(outputs[0][input_token_len:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompter(question):\n",
    "    prompt = f'''<|im_start|>system\n",
    "You are an advanced language model adept at interpreting and refining noisy or imperfect user inputs.\n",
    "Given user data, your task is to accurately extract the intended question and provide precise answers or predictions, even if the input contains errors or discontinuities.<|im_end|>\n",
    "<|im_start|>user\n",
    "{question}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "'''\n",
    "    # print(prompt)\n",
    "    return inference(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset/valid.json\", \"r\") as f:\n",
    "    valid_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "#     q = valid_data[i]['input_disfluent']\n",
    "#     a = valid_data[i]['output_original']\n",
    "\n",
    "#     print(\"LLM:\", prompter(q))\n",
    "#     print(\"ANS:\", a)\n",
    "#     print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data 1000, BLEU: 0.8239789905474549\n",
      "0.8239789905474549\n"
     ]
    }
   ],
   "source": [
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "tot_bleu = 0.\n",
    "model_io = []\n",
    "\n",
    "for idx, data in enumerate(valid_data):\n",
    "    ques = data['input_disfluent']\n",
    "    ref = data['output_original']\n",
    "    pred = prompter(ques)\n",
    "    bleu = bleu_metric.compute(predictions=[pred], references=[ref])['bleu']\n",
    "    tot_bleu += bleu\n",
    "    model_io.append({\"input_ques\": ques, \"output_pred\": pred, \"output_ground\": ref, \"bleu\": bleu})\n",
    "    print(f\"\\rData {idx+1}, BLEU: {tot_bleu/(idx+1)}\", end=\"\")\n",
    "\n",
    "model_io.append({\"AVG_BLEU\": tot_bleu/len(valid_data)})\n",
    "with open(\"outputs/valid_io_smollm.json\", \"w\") as f:\n",
    "    json.dump(model_io, f, indent=2)\n",
    "\n",
    "print()\n",
    "print(tot_bleu/len(valid_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset/train.json\", \"r\") as f:\n",
    "    train_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data 7182, BLEU: 0.8887369601868057"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'valid_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     model_io\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ques\u001b[39m\u001b[38;5;124m\"\u001b[39m: ques, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m: pred, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_ground\u001b[39m\u001b[38;5;124m\"\u001b[39m: ref, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbleu\u001b[39m\u001b[38;5;124m\"\u001b[39m: bleu})\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124mData \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, BLEU: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtot_bleu\u001b[38;5;241m/\u001b[39m(idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m model_io\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAVG_BLEU\u001b[39m\u001b[38;5;124m\"\u001b[39m: tot_bleu\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[43mvalid_data\u001b[49m)})\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs/train_io.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     16\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(model_io, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'valid_data' is not defined"
     ]
    }
   ],
   "source": [
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "tot_bleu = 0.\n",
    "model_io = []\n",
    "\n",
    "for idx, data in enumerate(train_data):\n",
    "    ques = data['input_disfluent']\n",
    "    ref = data['output_original']\n",
    "    pred = prompter(ques)\n",
    "    bleu = bleu_metric.compute(predictions=[pred], references=[ref])['bleu']\n",
    "    tot_bleu += bleu\n",
    "    model_io.append({\"input_ques\": ques, \"output_pred\": pred, \"output_ground\": ref, \"bleu\": bleu})\n",
    "    print(f\"\\rData {idx+1}, BLEU: {tot_bleu/(idx+1)}\", end=\"\")\n",
    "\n",
    "model_io.append({\"AVG_BLEU\": tot_bleu/len(train_data)})\n",
    "with open(\"outputs/train_io.json\", \"w\") as f:\n",
    "    json.dump(model_io, f, indent=2)\n",
    "\n",
    "print()\n",
    "print(tot_bleu/len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
