{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    # BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "import json\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import os\n",
    "\n",
    "@dataclass\n",
    "class DataClass:\n",
    "    MODEL_PATH = \"Qwen/Qwen2-0.5B-Instruct\"\n",
    "    MAX_LENGTH = 96\n",
    "    EPOCH = 3\n",
    "    LORA_RANK = 2\n",
    "    LORA_ALPHA = 4 * LORA_RANK\n",
    "    LORA_DROPOUT = 0.5\n",
    "    LORA_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "                    \"lm_head\"]\n",
    "    LR = 5e-5\n",
    "    ADAPTER_PATH = 'weights/LORA/checkpoint-1794'\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'mps'\n",
    "\n",
    "# Macbook MPS\n",
    "if DataClass.DEVICE == 'mps':\n",
    "    os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "    # Optional: Turnning off tokenizer parallelism to avoid stuck\n",
    "    # os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = AutoConfig.from_pretrained(\n",
    "    DataClass.MODEL_PATH,\n",
    "    trust_remote_code = True,\n",
    "    attn_implementation = 'eager', #'flash_attention_2'\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    DataClass.MODEL_PATH,\n",
    "    trust_remote_code = True\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# quant_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit = True,\n",
    "#     bnb_4bit_quant_type=\"n4f\",\n",
    "#     bnb4bit_compute_dtype=torch.bfloat16,\n",
    "#     bnb_4bit_use_double_quant=True\n",
    "# )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    DataClass.MODEL_PATH,\n",
    "    device_map=DataClass.DEVICE,\n",
    "    low_cpu_mem_usage=True,\n",
    "    # load_in_8bit=True,\n",
    "    # load_in_4bit=True,\n",
    "    attn_implementation='eager', #'flash_attention_2',\n",
    "    torch_dtype=torch.bfloat16, # NOTE: MPS does not support torch.bfloat16 finetuning\n",
    "    trust_remote_code=True,\n",
    "    # quantization_config=quant_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DataClass.ADAPTER_PATH:\n",
    "    model.load_adapter(DataClass.ADAPTER_PATH)\n",
    "    print(\"Adapter loaded from:\", DataClass.ADAPTER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(input_text):\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    outputs = model.generate(\n",
    "        # **input_ids\n",
    "        max_new_tokens=100,\n",
    "        do_sample=False,\n",
    "        num_beams=1,\n",
    "        temperature=None,\n",
    "        top_k=None,\n",
    "        top_p=None,\n",
    "        input_ids=input_ids['input_ids'].to(DataClass.DEVICE),\n",
    "        attention_mask=input_ids['attention_mask'].to(DataClass.DEVICE)\n",
    "    )\n",
    "    # Only generate output\n",
    "    input_token_len = input_ids['input_ids'].shape[-1]\n",
    "    return tokenizer.decode(outputs[0][input_token_len:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompter(question):\n",
    "    prompt = f'''<|im_start|>system\n",
    "You are an advanced language model adept at interpreting and refining noisy or imperfect user inputs.\n",
    "Given user data, your task is to accurately extract the intended question and provide precise answers or predictions, even if the input contains errors or discontinuities.<|im_end|>\n",
    "<|im_start|>user\n",
    "{question}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "'''\n",
    "    # print(prompt)\n",
    "    return inference(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset/valid.json\", \"r\") as f:\n",
    "    valid_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "#     q = valid_data[i]['input_disfluent']\n",
    "#     a = valid_data[i]['output_original']\n",
    "#     \n",
    "#     print(\"LLM:\", prompter(q))\n",
    "#     print(\"ANS:\", a)\n",
    "#     print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "tot_bleu = 0.\n",
    "model_io = []\n",
    "\n",
    "for idx, data in enumerate(valid_data):\n",
    "    ques = data['input_disfluent']\n",
    "    ref = data['output_original']\n",
    "    pred = prompter(ques)\n",
    "    bleu = bleu_metric.compute(predictions=[pred], references=[ref])['bleu']\n",
    "    tot_bleu += bleu\n",
    "    model_io.append({\"input_ques\": ques, \"output_pred\": pred, \"output_ground\": ref, \"bleu\": bleu})\n",
    "    print(f\"\\rData {idx+1}, BLEU: {tot_bleu/(idx+1)}\", end=\"\")\n",
    "\n",
    "model_io.append({\"AVG_BLEU\": tot_bleu/len(valid_data)})\n",
    "with open(\"outputs/valid_io.json\", \"w\") as f:\n",
    "    json.dump(model_io, f, indent=2)\n",
    "\n",
    "print()\n",
    "print(tot_bleu/len(valid_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset/train.json\", \"r\") as f:\n",
    "    train_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "tot_bleu = 0.\n",
    "model_io = []\n",
    "\n",
    "for idx, data in enumerate(train_data):\n",
    "    ques = data['input_disfluent']\n",
    "    ref = data['output_original']\n",
    "    pred = prompter(ques)\n",
    "    bleu = bleu_metric.compute(predictions=[pred], references=[ref])['bleu']\n",
    "    tot_bleu += bleu\n",
    "    model_io.append({\"input_ques\": ques, \"output_pred\": pred, \"output_ground\": ref, \"bleu\": bleu})\n",
    "    print(f\"\\rData {idx+1}, BLEU: {tot_bleu/(idx+1)}\", end=\"\")\n",
    "\n",
    "model_io.append({\"AVG_BLEU\": tot_bleu/len(train_data)})\n",
    "with open(\"outputs/train_io.json\", \"w\") as f:\n",
    "    json.dump(model_io, f, indent=2)\n",
    "\n",
    "print()\n",
    "print(tot_bleu/len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
