{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ohi/Documents/GitHub/ChataRewrite/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    # BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "import json\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import os\n",
    "\n",
    "@dataclass\n",
    "class DataClass:\n",
    "    MODEL_PATH = [\"weights/RegularFinetune/checkpoint-897\", \"Qwen/Qwen2-0.5B-Instruct\"][0]\n",
    "    MAX_LENGTH = 96\n",
    "    EPOCH = 3\n",
    "    LORA_RANK = 2\n",
    "    LORA_ALPHA = 2 * LORA_RANK\n",
    "    LORA_DROPOUT = 0.5\n",
    "    LORA_MODULES = [\"o_proj\", \"qjv_proj\", \"gate_up_proj\"]\n",
    "    LR = 5e-5\n",
    "    MODEL_SAVE_FOLDER = '/content/drive/MyDrive/weights'\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'mps'\n",
    "\n",
    "# Macbook MPS\n",
    "if DataClass.DEVICE == 'mps':\n",
    "    os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "    # Optional: Turnning off tokenizer parallelism to avoid stuck\n",
    "    # os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = AutoConfig.from_pretrained(\n",
    "    DataClass.MODEL_PATH,\n",
    "    trust_remote_code = True,\n",
    "    attn_implementation = 'eager', #'flash_attention_2'\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    DataClass.MODEL_PATH,\n",
    "    trust_remote_code = True\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# quant_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit = True,\n",
    "#     bnb_4bit_quant_type=\"n4f\",\n",
    "#     bnb4bit_compute_dtype=torch.bfloat16,\n",
    "#     bnb_4bit_use_double_quant=True\n",
    "# )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    DataClass.MODEL_PATH,\n",
    "    device_map=DataClass.DEVICE,\n",
    "    low_cpu_mem_usage=True,\n",
    "    # load_in_8bit=True,\n",
    "    # load_in_4bit=True,\n",
    "    attn_implementation='eager', #'flash_attention_2',\n",
    "    torch_dtype=torch.bfloat16, # NOTE: MPS does not support torch.bfloat16 finetuning\n",
    "    trust_remote_code=True,\n",
    "    # quantization_config=quant_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(input_text):\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    outputs = model.generate(\n",
    "        # **input_ids\n",
    "        max_new_tokens=100,\n",
    "        do_sample=False,\n",
    "        num_beams=1,\n",
    "        temperature=None,\n",
    "        top_k=None,\n",
    "        top_p=None,\n",
    "        input_ids=input_ids['input_ids'].to(DataClass.DEVICE),\n",
    "        attention_mask=input_ids['attention_mask'].to(DataClass.DEVICE)\n",
    "    )\n",
    "    # Only generate output\n",
    "    input_token_len = input_ids['input_ids'].shape[-1]\n",
    "    return tokenizer.decode(outputs[0][input_token_len:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompter(question):\n",
    "    prompt = f'''<|im_start|>system\n",
    "You are an advanced language model adept at interpreting and refining noisy or imperfect user inputs.\n",
    "Given user data, your task is to accurately extract the intended question and provide precise answers or predictions, even if the input contains errors or discontinuities.<|im_end|>\n",
    "<|im_start|>user\n",
    "{question}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "'''\n",
    "    # print(prompt)\n",
    "    return inference(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset/valid.json\", \"r\") as f:\n",
    "    valid_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM: What did the government want Thoreau to do?\n",
      "ANS: What did the government want Thoreau to do?\n",
      "---\n",
      "LLM: What makes the Wells Fargo Center stand out?\n",
      "ANS: What makes the Wells Fargo Center stand out?\n",
      "---\n",
      "LLM: What was the Colonia Agrippina's original name?\n",
      "ANS: What was the Colonia Agrippina's original name?\n",
      "---\n",
      "LLM: Extended authorization networking benefits helped those that could not connect to what platform?\n",
      "ANS: Extended networking benefits helped those that could not connect to what platform? \n",
      "---\n",
      "LLM: Who is the emphasis on when there is a private finance initiative?\n",
      "ANS: Who is the emphasis on when there is a private finance initiative?\n",
      "---\n",
      "LLM:  What dynasties inspired the Chinese-like elements of Kublai's government?\n",
      "ANS: What dynasties inspired the Chinese-like elements of Kublai's government?\n",
      "---\n",
      "LLM: What is the density of all primes compatible with a modulo 9?\n",
      "ANS: What is the density of all primes compatible with a modulo 9?\n",
      "---\n",
      "LLM: What did European empires rely on to supply them with resources?\n",
      "ANS: What did European empires rely on to supply them with resources?\n",
      "---\n",
      "LLM: What did Karlen and Singer present to the US senate?\n",
      "ANS: What did Karlen and Singer present to the US senate?\n",
      "---\n",
      "LLM: What is the current status of the Haensch study?\n",
      "ANS: What is the current status of the Haensch study?\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    q = valid_data[i]['input_disfluent']\n",
    "    a = valid_data[i]['output_original']\n",
    "\n",
    "    print(\"LLM:\", prompter(q))\n",
    "    print(\"ANS:\", a)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data 100, BLEU: 0.8977773798236173\n",
      "0.8977773798236173\n"
     ]
    }
   ],
   "source": [
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "tot_bleu = 0.\n",
    "model_io = []\n",
    "\n",
    "for idx, data in enumerate(valid_data[:100]):\n",
    "    ques = data['input_disfluent']\n",
    "    ref = data['output_original']\n",
    "    pred = prompter(ques)\n",
    "    bleu = bleu_metric.compute(predictions=[pred], references=[ref])['bleu']\n",
    "    tot_bleu += bleu\n",
    "    model_io.append({\"input_ques\": ques, \"output_pred\": pred, \"output_ground\": ref, \"bleu\": bleu})\n",
    "    print(f\"\\rData {idx+1}, BLEU: {tot_bleu/(idx+1)}\", end=\"\")\n",
    "\n",
    "model_io.append({\"AVG_BLEU\": tot_bleu/len(valid_data)})\n",
    "with open(\"outputs/valid_io.json\", \"w\") as f:\n",
    "    json.dump(model_io, f, indent=2)\n",
    "\n",
    "print()\n",
    "print(tot_bleu/len(valid_data[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset/train.json\", \"r\") as f:\n",
    "    train_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data 100, BLEU: 0.9480560108343412\n",
      "0.9480560108343412\n"
     ]
    }
   ],
   "source": [
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "tot_bleu = 0.\n",
    "model_io = []\n",
    "\n",
    "for idx, data in enumerate(train_data[:100]):\n",
    "    ques = data['input_disfluent']\n",
    "    ref = data['output_original']\n",
    "    pred = prompter(ques)\n",
    "    bleu = bleu_metric.compute(predictions=[pred], references=[ref])['bleu']\n",
    "    tot_bleu += bleu\n",
    "    model_io.append({\"input_ques\": ques, \"output_pred\": pred, \"output_ground\": ref, \"bleu\": bleu})\n",
    "    print(f\"\\rData {idx+1}, BLEU: {tot_bleu/(idx+1)}\", end=\"\")\n",
    "\n",
    "model_io.append({\"AVG_BLEU\": tot_bleu/len(valid_data)})\n",
    "with open(\"outputs/train_io.json\", \"w\") as f:\n",
    "    json.dump(model_io, f, indent=2)\n",
    "\n",
    "print()\n",
    "print(tot_bleu/len(train_data[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
