{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    # BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "import json\n",
    "import os\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    device = 'cpu'\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "    # Macbook MPS\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = 'mps'\n",
    "    return device\n",
    "\n",
    "\n",
    "# Macbook MPS\n",
    "if get_device() == 'mps':\n",
    "    os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "    # Optional: Turnning off tokenizer parallelism to avoid stuck\n",
    "    # os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = AutoConfig.from_pretrained(\n",
    "    \"Qwen/Qwen2-0.5B-Instruct\",\n",
    "    trust_remote_code = True,\n",
    "    attn_implementation = 'eager', #'flash_attention_2'\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Qwen/Qwen2-0.5B-Instruct\",\n",
    "    trust_remote_code = True\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# quant_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit = True,\n",
    "#     bnb_4bit_quant_type=\"n4f\",\n",
    "#     bnb4bit_compute_dtype=torch.bfloat16,\n",
    "#     bnb_4bit_use_double_quant=True\n",
    "# )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2-0.5B-Instruct\",\n",
    "    device_map=get_device(),\n",
    "    low_cpu_mem_usage=True,\n",
    "    # load_in_8bit=True,\n",
    "    # load_in_4bit=True,\n",
    "    attn_implementation='eager', #'flash_attention_2',\n",
    "    torch_dtype=torch.bfloat16, # NOTE: MPS does not support torch.bfloat16 finetuning\n",
    "    trust_remote_code=True,\n",
    "    # quantization_config=quant_config\n",
    ")\n",
    "\n",
    "model.load_adapter('weights/LORA/checkpoint-1794')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(input_text):\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    outputs = model.generate(\n",
    "        # **input_ids\n",
    "        max_new_tokens=100,\n",
    "        # do_sample=False,\n",
    "        # num_beams=1,\n",
    "        # temperature=None,\n",
    "        # top_k=None,\n",
    "        # top_p=None,\n",
    "        input_ids=input_ids['input_ids'].to(get_device()),\n",
    "        attention_mask=input_ids['attention_mask'].to(get_device())\n",
    "    )\n",
    "    # Only generate output\n",
    "    input_token_len = input_ids['input_ids'].shape[-1]\n",
    "    return tokenizer.decode(outputs[0][input_token_len:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompter(question):\n",
    "    prompt = f'''<|im_start|>system\n",
    "You are an advanced language model adept at interpreting and refining noisy or imperfect user inputs.\n",
    "Given user data, your task is to accurately extract the intended question and provide precise answers or predictions, even if the input contains errors or discontinuities.<|im_end|>\n",
    "<|im_start|>user\n",
    "{question}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "'''\n",
    "    # print(prompt)\n",
    "    return inference(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What Goldman Sachs CEO is also an alumni of the University of Chicago?'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The question has to be given here\n",
    "\n",
    "prompter(\"What Microsoft err I mean Goldman Sachs CEO is also an alumni of the University of Chicago?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What French no British General negotiated at Montreal?', 'Which president signed no did away with the Christian position in the curriculum?', 'What president eliminated the Christian position in the curriculum?']\n"
     ]
    }
   ],
   "source": [
    "with open(\"inference.json\", \"r\") as f:\n",
    "    inp_data = json.load(f)\n",
    "\n",
    "# print(inp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "for data in inp_data:\n",
    "    outputs.append(prompter(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What British General negotiated at Montreal?', 'Which president did away with the Christian position in the curriculum?', 'What did the President eliminate the Christian position from the curriculum?']\n"
     ]
    }
   ],
   "source": [
    "# print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"outputs/inference_output.json\", \"w\") as f:\n",
    "    json.dump(outputs, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
