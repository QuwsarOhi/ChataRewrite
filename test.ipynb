{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    # BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "import json\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.4.0-cp312-none-macosx_11_0_arm64.whl.metadata (26 kB)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting peft\n",
      "  Using cached peft-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting bitsandbytes\n",
      "  Using cached bitsandbytes-0.42.0-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting evaluate\n",
      "  Using cached evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting datasets\n",
      "  Using cached datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.15.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting typing-extensions>=4.8.0 (from torch)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting sympy (from torch)\n",
      "  Using cached sympy-1.13.2-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting setuptools (from torch)\n",
      "  Using cached setuptools-73.0.1-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Using cached huggingface_hub-0.24.6-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting numpy>=1.17 (from transformers)\n",
      "  Using cached numpy-2.1.0-cp312-cp312-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Using cached PyYAML-6.0.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.7.24-cp312-cp312-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting requests (from transformers)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.4.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Using cached tokenizers-0.19.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.12/site-packages (from peft) (6.0.0)\n",
      "Collecting accelerate>=0.21.0 (from peft)\n",
      "  Using cached accelerate-0.33.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting scipy (from bitsandbytes)\n",
      "  Using cached scipy-1.14.1-cp312-cp312-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "Collecting dill (from evaluate)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from evaluate)\n",
      "  Using cached pandas-2.2.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (19 kB)\n",
      "Collecting xxhash (from evaluate)\n",
      "  Using cached xxhash-3.5.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from evaluate)\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Using cached pyarrow-17.0.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.3 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Using cached aiohttp-3.10.5-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.5 kB)\n",
      "Collecting numpy>=1.17 (from transformers)\n",
      "  Using cached numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Using cached aiohappyeyeballs-2.4.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
      "  Using cached attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Using cached frozenlist-1.4.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Using cached multidict-6.0.5-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Using cached yarl-1.9.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (31 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->transformers)\n",
      "  Using cached charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers)\n",
      "  Using cached idna-3.8-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Using cached urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
      "  Using cached certifi-2024.7.4-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Using cached MarkupSafe-2.1.5-cp312-cp312-macosx_10_9_universal2.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->evaluate)\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->evaluate)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Using cached torch-2.4.0-cp312-none-macosx_11_0_arm64.whl (62.1 MB)\n",
      "Using cached transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
      "Using cached peft-0.12.0-py3-none-any.whl (296 kB)\n",
      "Using cached bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n",
      "Using cached evaluate-0.4.2-py3-none-any.whl (84 kB)\n",
      "Using cached datasets-2.21.0-py3-none-any.whl (527 kB)\n",
      "Using cached accelerate-0.33.0-py3-none-any.whl (315 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Using cached aiohttp-3.10.5-cp312-cp312-macosx_11_0_arm64.whl (389 kB)\n",
      "Using cached huggingface_hub-0.24.6-py3-none-any.whl (417 kB)\n",
      "Using cached numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl (13.7 MB)\n",
      "Using cached pyarrow-17.0.0-cp312-cp312-macosx_11_0_arm64.whl (27.2 MB)\n",
      "Using cached PyYAML-6.0.2-cp312-cp312-macosx_11_0_arm64.whl (173 kB)\n",
      "Using cached regex-2024.7.24-cp312-cp312-macosx_11_0_arm64.whl (279 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached safetensors-0.4.4-cp312-cp312-macosx_11_0_arm64.whl (381 kB)\n",
      "Using cached tokenizers-0.19.1-cp312-cp312-macosx_11_0_arm64.whl (2.4 MB)\n",
      "Using cached tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached filelock-3.15.4-py3-none-any.whl (16 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Using cached networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "Using cached pandas-2.2.2-cp312-cp312-macosx_11_0_arm64.whl (11.3 MB)\n",
      "Using cached scipy-1.14.1-cp312-cp312-macosx_14_0_arm64.whl (23.1 MB)\n",
      "Using cached setuptools-73.0.1-py3-none-any.whl (2.3 MB)\n",
      "Using cached sympy-1.13.2-py3-none-any.whl (6.2 MB)\n",
      "Using cached xxhash-3.5.0-cp312-cp312-macosx_11_0_arm64.whl (30 kB)\n",
      "Using cached aiohappyeyeballs-2.4.0-py3-none-any.whl (12 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "Using cached certifi-2024.7.4-py3-none-any.whl (162 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl (119 kB)\n",
      "Using cached frozenlist-1.4.1-cp312-cp312-macosx_11_0_arm64.whl (51 kB)\n",
      "Using cached idna-3.8-py3-none-any.whl (66 kB)\n",
      "Using cached MarkupSafe-2.1.5-cp312-cp312-macosx_10_9_universal2.whl (18 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached multidict-6.0.5-cp312-cp312-macosx_11_0_arm64.whl (29 kB)\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Using cached urllib3-2.2.2-py3-none-any.whl (121 kB)\n",
      "Using cached yarl-1.9.4-cp312-cp312-macosx_11_0_arm64.whl (79 kB)\n",
      "Installing collected packages: pytz, mpmath, xxhash, urllib3, tzdata, typing-extensions, tqdm, sympy, setuptools, safetensors, regex, pyyaml, numpy, networkx, multidict, MarkupSafe, idna, fsspec, frozenlist, filelock, dill, charset-normalizer, certifi, attrs, aiohappyeyeballs, yarl, scipy, requests, pyarrow, pandas, multiprocess, jinja2, aiosignal, torch, huggingface-hub, bitsandbytes, aiohttp, tokenizers, accelerate, transformers, datasets, peft, evaluate\n",
      "Successfully installed MarkupSafe-2.1.5 accelerate-0.33.0 aiohappyeyeballs-2.4.0 aiohttp-3.10.5 aiosignal-1.3.1 attrs-24.2.0 bitsandbytes-0.42.0 certifi-2024.7.4 charset-normalizer-3.3.2 datasets-2.21.0 dill-0.3.8 evaluate-0.4.2 filelock-3.15.4 frozenlist-1.4.1 fsspec-2024.6.1 huggingface-hub-0.24.6 idna-3.8 jinja2-3.1.4 mpmath-1.3.0 multidict-6.0.5 multiprocess-0.70.16 networkx-3.3 numpy-1.26.4 pandas-2.2.2 peft-0.12.0 pyarrow-17.0.0 pytz-2024.1 pyyaml-6.0.2 regex-2024.7.24 requests-2.32.3 safetensors-0.4.4 scipy-1.14.1 setuptools-73.0.1 sympy-1.13.2 tokenizers-0.19.1 torch-2.4.0 tqdm-4.66.5 transformers-4.44.2 typing-extensions-4.12.2 tzdata-2024.1 urllib3-2.2.2 xxhash-3.5.0 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch transformers peft bitsandbytes evaluate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import os\n",
    "\n",
    "@dataclass\n",
    "class DataClass:\n",
    "    MODEL_PATH = [\"weights/checkpoint-897\", \"Qwen/Qwen2-0.5B-Instruct\"][0]\n",
    "    MAX_LENGTH = 96\n",
    "    EPOCH = 3\n",
    "    LORA_RANK = 2\n",
    "    LORA_ALPHA = 2 * LORA_RANK\n",
    "    LORA_DROPOUT = 0.5\n",
    "    LORA_MODULES = [\"o_proj\", \"qjv_proj\", \"gate_up_proj\"]\n",
    "    LR = 5e-5\n",
    "    MODEL_SAVE_FOLDER = '/content/drive/MyDrive/weights'\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'mps'\n",
    "\n",
    "# Macbook MPS\n",
    "if DataClass.DEVICE == 'mps':\n",
    "    os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = AutoConfig.from_pretrained(\n",
    "    DataClass.MODEL_PATH,\n",
    "    trust_remote_code = True,\n",
    "    attn_implementation = 'eager', #'flash_attention_2'\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    DataClass.MODEL_PATH,\n",
    "    trust_remote_code = True\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# quant_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit = True,\n",
    "#     bnb_4bit_quant_type=\"n4f\",\n",
    "#     bnb4bit_compute_dtype=torch.bfloat16,\n",
    "#     bnb_4bit_use_double_quant=True\n",
    "# )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    DataClass.MODEL_PATH,\n",
    "    device_map=DataClass.DEVICE,\n",
    "    low_cpu_mem_usage=True,\n",
    "    # load_in_8bit=True,\n",
    "    # load_in_4bit=True,\n",
    "    attn_implementation='eager', #'flash_attention_2',\n",
    "    torch_dtype=torch.bfloat16, # NOTE: MPS does not support torch.bfloat16 finetuning\n",
    "    trust_remote_code=True,\n",
    "    # quantization_config=quant_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(input_text):\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    outputs = model.generate(\n",
    "        # **input_ids\n",
    "        max_new_tokens=100,\n",
    "        do_sample=False,\n",
    "        num_beams=1,\n",
    "        temperature=None,\n",
    "        top_k=None,\n",
    "        top_p=None,\n",
    "        input_ids=input_ids['input_ids'].to(DataClass.DEVICE),\n",
    "        attention_mask=input_ids['attention_mask'].to(DataClass.DEVICE)\n",
    "    )\n",
    "    # Only generate output\n",
    "    input_token_len = input_ids['input_ids'].shape[-1]\n",
    "    return tokenizer.decode(outputs[0][input_token_len:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompter(question):\n",
    "    prompt = f'''<|im_start|>system\n",
    "You are an advanced language model adept at interpreting and refining noisy or imperfect user inputs.\n",
    "Given user data, your task is to accurately extract the intended question and provide precise answers or predictions, even if the input contains errors or discontinuities.<|im_end|>\n",
    "<|im_start|>user\n",
    "{question}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "'''\n",
    "    # print(prompt)\n",
    "    return inference(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"valid.json\", \"r\") as f:\n",
    "    valid_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM: What did the government want Thoreau to do?\n",
      "ANS: What did the government want Thoreau to do?\n",
      "---\n",
      "LLM: What makes the Wells Fargo Center stand out?\n",
      "ANS: What makes the Wells Fargo Center stand out?\n",
      "---\n",
      "LLM: What was the Colonia Agrippina's original name?\n",
      "ANS: What was the Colonia Agrippina's original name?\n",
      "---\n",
      "LLM: Extended authorization networking benefits helped those that could not connect to what platform?\n",
      "ANS: Extended networking benefits helped those that could not connect to what platform? \n",
      "---\n",
      "LLM: Who is the emphasis on when there is a private finance initiative?\n",
      "ANS: Who is the emphasis on when there is a private finance initiative?\n",
      "---\n",
      "LLM:  What dynasties inspired the Chinese-like elements of Kublai's government?\n",
      "ANS: What dynasties inspired the Chinese-like elements of Kublai's government?\n",
      "---\n",
      "LLM: What is the density of all primes compatible with a modulo 9?\n",
      "ANS: What is the density of all primes compatible with a modulo 9?\n",
      "---\n",
      "LLM: What did European empires rely on to supply them with resources?\n",
      "ANS: What did European empires rely on to supply them with resources?\n",
      "---\n",
      "LLM: What did Karlen and Singer present to the US senate?\n",
      "ANS: What did Karlen and Singer present to the US senate?\n",
      "---\n",
      "LLM: What is the current status of the Haensch study?\n",
      "ANS: What is the current status of the Haensch study?\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    q = valid_data[i]['input_disfluent']\n",
    "    a = valid_data[i]['output_original']\n",
    "\n",
    "    print(\"LLM:\", prompter(q))\n",
    "    print(\"ANS:\", a)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8716455833892353\n"
     ]
    }
   ],
   "source": [
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "tot_bleu = 0.\n",
    "model_io = []\n",
    "\n",
    "for data in valid_data:\n",
    "    ques = data['input_disfluent']\n",
    "    ref = data['output_original']\n",
    "    pred = prompter(ques)\n",
    "    tot_bleu += bleu_metric.compute(predictions=[pred], references=[ref])['bleu']\n",
    "    model_io.append({\"input_ques\": ques, \"output_pred\": pred, \"output_ground\": ref})\n",
    "\n",
    "with open(\"model_io.json\", \"w\") as f:\n",
    "    json.dump(model_io, f, indent=2)\n",
    "\n",
    "print(tot_bleu/len(valid_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "tot_bleu = 0.\n",
    "\n",
    "with open(\"train.json\", \"r\") as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "for data in train_data:\n",
    "    ques = data['input_disfluent']\n",
    "    ref = data['output_original']\n",
    "    pred = prompter(ques)\n",
    "    tot_bleu += bleu_metric.compute(predictions=[pred], references=[ref])['bleu']\n",
    "\n",
    "print(tot_bleu/len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
